{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db91fdb",
   "metadata": {},
   "source": [
    "### ================================================================\n",
    "### AI Tutor Pipeline - Step 3: Personalized Learning Path Builder\n",
    "### ---------------------------------------------------------------\n",
    "### This script:\n",
    "###   - Matches curriculum topics to PDF content\n",
    "###   - Simulates (or loads) quiz results for a user\n",
    "###   - Computes the user learning zone (beginner/advanced, fast/slow)\n",
    "###   - Calculates review pace per topic\n",
    "###   - Outputs a personalized learning path (JSON) with pacing info\n",
    "### ================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b3e3c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776447b",
   "metadata": {},
   "source": [
    "### ========== 1. Load Curriculum Skeleton =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f98aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"json_files/learning_path_skeleton_5th.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    skeleton = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8225f9e3",
   "metadata": {},
   "source": [
    "### ========== 2. Chunk PDF =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ebba9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"files/science-textbook-grade-5.pdf\"   # <-- update this!\n",
    "loader = PyMuPDFLoader(PDF_PATH)\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)  # List of Document objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f2ec7",
   "metadata": {},
   "source": [
    "### ========== 3. Content Matching Functions =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d1a7720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    return re.sub(r'[^a-z0-9 ]', '', text.lower())\n",
    "\n",
    "def fuzzy_ratio(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def find_matching_content(topic, chunks):\n",
    "    norm_title = normalize(topic)\n",
    "    candidates = []\n",
    "    # 1. Exact normalized substring match\n",
    "    for c in chunks:\n",
    "        norm_chunk = normalize(c.page_content)\n",
    "        if norm_title in norm_chunk:\n",
    "            return c.page_content\n",
    "    # 2. Partial word overlap: at least 60% of words\n",
    "    title_words = set(norm_title.split())\n",
    "    for c in chunks:\n",
    "        chunk_words = set(normalize(c.page_content).split())\n",
    "        if len(title_words & chunk_words) >= max(1, int(0.6 * len(title_words))):\n",
    "            candidates.append(c.page_content)\n",
    "    if candidates:\n",
    "        return max(candidates, key=len)\n",
    "    # 3. Fuzzy matching: best ratio\n",
    "    best_score = 0\n",
    "    best_chunk = \"\"\n",
    "    for c in chunks:\n",
    "        norm_chunk = normalize(c.page_content)\n",
    "        score = fuzzy_ratio(norm_title, norm_chunk)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = c.page_content\n",
    "    if best_score > 0.5:\n",
    "        return best_chunk\n",
    "    # 4. Fallback\n",
    "    print(f\"Warning: No content found for topic '{topic}'\")\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03d6714",
   "metadata": {},
   "source": [
    "### ========== 4. Topic Extraction & Mapping =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "258f9a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topics(skeleton):\n",
    "    topics = []\n",
    "    for section in skeleton[\"sections\"]:\n",
    "        topics.append(section[\"title\"])  # Always add chapter/section\n",
    "        for sub in section.get(\"subsections\", []):\n",
    "            topics.append(sub[\"title\"])\n",
    "            for st in sub.get(\"sub_titles\", []):\n",
    "                topics.append(st)\n",
    "    return topics  # NO set() so duplicate sub-title topics show in both chapter & sub\n",
    "\n",
    "topics = extract_topics(skeleton)\n",
    "dependency_map = {t: [] for t in topics}  # No prereqs for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c1e70f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_topic_text_map(topics, chunks):\n",
    "#     topic_text_map = {}\n",
    "#     for topic in topics:\n",
    "#         matched_chunks = [c.page_content for c in chunks if topic.lower() in c.page_content.lower()]\n",
    "#         topic_text_map[topic] = \"\\n\".join(matched_chunks)\n",
    "#     return topic_text_map\n",
    "\n",
    "# topic_text_map = get_topic_text_map(topics, chunks)\n",
    "\n",
    "def get_topic_text_map(topics, chunks):\n",
    "    topic_text_map = {}\n",
    "    for topic in topics:\n",
    "        topic_text_map[topic] = find_matching_content(topic, chunks)\n",
    "    return topic_text_map\n",
    "\n",
    "topic_text_map = get_topic_text_map(topics, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b4036",
   "metadata": {},
   "source": [
    "### ========== 5. Content Type Analysis =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ecd8945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_content_types(text):\n",
    "    code_lines = 0\n",
    "    math_lines = 0\n",
    "    table_lines = 0\n",
    "    word_count = 0\n",
    "    for line in text.splitlines():\n",
    "        line_stripped = line.strip()\n",
    "        # Code detection (SQL, Python, general)\n",
    "        if (line_stripped.upper().startswith((\"SELECT\", \"INSERT\", \"UPDATE\", \"DELETE\", \"CREATE\", \"ALTER\", \"DROP\", \"def \", \"class \", \"import \", \"#\", \"--\")) or\n",
    "            line_stripped.endswith(\";\") or\n",
    "            line_stripped.startswith(\"```\")):\n",
    "            code_lines += 1\n",
    "        # Math detection\n",
    "        if (any(s in line_stripped for s in (\"$\", \"\\\\(\", \"\\\\)\", \"=\", \"+\", \"-\", \"*\", \"/\")) and\n",
    "            sum(1 for c in line_stripped if c in \"=+-*/\") >= 2):\n",
    "            math_lines += 1\n",
    "        # Table detection\n",
    "        if (line_stripped.count('|') >= 2 or line_stripped.count(',') >= 4 or '\\t' in line_stripped):\n",
    "            table_lines += 1\n",
    "        word_count += len(line_stripped.split())\n",
    "    return word_count, code_lines, math_lines, table_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0a10fe",
   "metadata": {},
   "source": [
    "### ========== 6. Simulate (or Load) Quiz Results =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a0949f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"quiz_files/filtered_quiz_5th.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    quiz_data = json.load(f)\n",
    "questions = quiz_data[\"quiz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4e6020f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_answers(questions, topics_to_get_wrong=None, randomize=False):\n",
    "    answers = []\n",
    "    for q in questions:\n",
    "        topic = q.get(\"subsection_title\") or q.get(\"sub_title\") or q.get(\"section_title\") or \"Unknown\"\n",
    "        if randomize:\n",
    "            chosen = random.choice(['A', 'B', 'C', 'D'])\n",
    "        elif topics_to_get_wrong and topic in topics_to_get_wrong:\n",
    "            options = ['A', 'B', 'C', 'D']\n",
    "            if q['answer'] in options:\n",
    "                options.remove(q['answer'])\n",
    "            chosen = random.choice(options)\n",
    "        else:\n",
    "            chosen = q['answer']\n",
    "        answers.append({\n",
    "            \"question\": q[\"question\"],\n",
    "            \"topic\": topic,\n",
    "            \"selected_answer\": chosen,\n",
    "            \"correct_answer\": q[\"answer\"],\n",
    "            \"correct\": (chosen == q[\"answer\"]),\n",
    "            \"difficulty\": q.get(\"difficulty\", \"unknown\")\n",
    "        })\n",
    "    return answers\n",
    "\n",
    "user_quiz_results = generate_user_answers(questions, randomize=True)  # Set to False for real user input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c63c8",
   "metadata": {},
   "source": [
    "### ========== 7. User Zone Inference =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a3c007f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quiz score: 80\n",
      "User zone: fast-learner and advanced\n"
     ]
    }
   ],
   "source": [
    "def learning_zone(aptitude, quiz, threshold=70):\n",
    "    if aptitude >= threshold and quiz >= threshold:\n",
    "        return \"fast-learner and advanced\"\n",
    "    elif aptitude >= threshold and quiz < threshold:\n",
    "        return \"fast-learner and beginner\"\n",
    "    elif aptitude < threshold and quiz >= threshold:\n",
    "        return \"slow-learner and advanced\"\n",
    "    else:\n",
    "        return \"slow-learner and beginner\"\n",
    "\n",
    "correct = sum(1 for q in user_quiz_results if q[\"correct\"])\n",
    "# quiz_score = correct / len(user_quiz_results) * 100\n",
    "quiz_score = 80\n",
    "print(\"Quiz score:\", quiz_score)\n",
    "aptitude_score = 80  # Or real user score\n",
    "user_zone = learning_zone(aptitude_score, quiz_score)\n",
    "print(\"User zone:\", user_zone)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb16bf7",
   "metadata": {},
   "source": [
    "### ========== 8. Pace Estimation Functions =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6bd1fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pace(word_count, code_lines, math_lines, table_lines, user_zone, missed_mcqs=0):\n",
    "    base_wpm = 200\n",
    "    base_time_min = (word_count / base_wpm) + (code_lines * 0.7) + (math_lines * 1) + (table_lines * 1)\n",
    "    zone_multiplier = {\n",
    "        \"fast-learner and advanced\": 0.7,\n",
    "        \"fast-learner and beginner\": 1.0,\n",
    "        \"slow-learner and advanced\": 1.15,\n",
    "        \"slow-learner and beginner\": 1.3\n",
    "    }[user_zone]\n",
    "    review_time = missed_mcqs * 5\n",
    "    total_minutes = base_time_min * zone_multiplier + review_time\n",
    "    if total_minutes < 60:\n",
    "        return f\"{round(total_minutes)} min\"\n",
    "    elif total_minutes < 8 * 60:\n",
    "        return f\"{round(total_minutes/60,1)} hours\"\n",
    "    else:\n",
    "        return f\"{round(total_minutes/60/8,1)} days\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8f1abf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prereqs(topic, deps_map, seen=None):\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    for pre in deps_map.get(topic, []):\n",
    "        if pre not in seen:\n",
    "            seen.add(pre)\n",
    "            get_prereqs(pre, deps_map, seen)\n",
    "    return seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bbc2ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_learning_path_with_prereqs(quiz_results, user_zone, deps_map, topic_text_map):\n",
    "    perf = defaultdict(list)\n",
    "    for q in quiz_results:\n",
    "        perf[q[\"topic\"]].append(q)\n",
    "    review_needed = set()\n",
    "    missed_mcq_map = {}\n",
    "    for topic, qs in perf.items():\n",
    "        missed = [q for q in qs if not q[\"correct\"]]\n",
    "        if missed:\n",
    "            review_needed.add(topic)\n",
    "            review_needed.update(get_prereqs(topic, deps_map))\n",
    "            missed_mcq_map[topic] = len(missed)\n",
    "    learning_path = []\n",
    "    for topic in deps_map.keys():\n",
    "        text = topic_text_map.get(topic, \"\")\n",
    "        word_count, code_lines, math_lines, table_lines = detect_content_types(text)\n",
    "        missed_mcqs = missed_mcq_map.get(topic, 0)\n",
    "        needs_review = topic in review_needed\n",
    "        pace = calculate_pace(word_count, code_lines, math_lines, table_lines, user_zone, missed_mcqs)\n",
    "        learning_path.append({\n",
    "            \"topic\": topic,\n",
    "            \"pace\": pace,\n",
    "            \"review_needed\": needs_review,\n",
    "            \"missed_mcqs\": missed_mcqs,\n",
    "            \"word_count\": word_count,\n",
    "            \"code_lines\": code_lines,\n",
    "            \"math_lines\": math_lines,\n",
    "            \"table_lines\": table_lines\n",
    "        })\n",
    "    return learning_path\n",
    "\n",
    "learning_path = build_learning_path_with_prereqs(\n",
    "    user_quiz_results,\n",
    "    user_zone,\n",
    "    dependency_map,\n",
    "    topic_text_map\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d22580",
   "metadata": {},
   "source": [
    "### ========== 9. Output Personalized Learning Path =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7f5768e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(learning_path)\n",
    "# print(df)\n",
    "learning_path_payload = {\n",
    "    \"user_zone\": user_zone,\n",
    "    \"learning_path\": learning_path\n",
    "}\n",
    "with open(\"path/learning_path_personalized_5th.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(learning_path_payload, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72d068",
   "metadata": {},
   "source": [
    "### ========== 10. Chapter-Wise Pace Rollup =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fa83220d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHAPTER-WISE ESTIMATED PACE:\n",
      "                         chapter    pace\n",
      "0     Structure of Living Things   8 min\n",
      "1  Plant Structure and Functions  13 min\n",
      "2             Human Body Systems  21 min\n",
      "3                  Earth's Water   5 min\n",
      "4                Earth's Weather   6 min\n",
      "5               The Solar System   9 min\n",
      "6                Types of Matter  16 min\n",
      "7              Changes in Matter   4 min\n"
     ]
    }
   ],
   "source": [
    "def pace_to_minutes(pace_str):\n",
    "    if \"min\" in pace_str:\n",
    "        return int(pace_str.split()[0])\n",
    "    elif \"hour\" in pace_str:\n",
    "        return int(float(pace_str.split()[0]) * 60)\n",
    "    elif \"day\" in pace_str:\n",
    "        return int(float(pace_str.split()[0]) * 8 * 60)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "chapter_times = []\n",
    "for section in skeleton[\"sections\"]:\n",
    "    chapter_title = section[\"title\"]\n",
    "    total_minutes = 0\n",
    "\n",
    "    # --- NEW: Chapter-level content (section title itself)\n",
    "    topic_row = next((t for t in learning_path if t[\"topic\"] == chapter_title), None)\n",
    "    if topic_row:\n",
    "        total_minutes += pace_to_minutes(topic_row[\"pace\"])\n",
    "\n",
    "    for sub in section.get(\"subsections\", []):\n",
    "        topic = sub.get(\"title\")\n",
    "        topic_row = next((t for t in learning_path if t[\"topic\"] == topic), None)\n",
    "        if topic_row:\n",
    "            total_minutes += pace_to_minutes(topic_row[\"pace\"])\n",
    "        # If you want to include sub_titles, you can loop those here too!\n",
    "\n",
    "    if total_minutes < 60:\n",
    "        pace_str = f\"{total_minutes} min\"\n",
    "    elif total_minutes < 8 * 60:\n",
    "        pace_str = f\"{round(total_minutes / 60, 1)} hours\"\n",
    "    else:\n",
    "        pace_str = f\"{round(total_minutes / 60 / 8, 1)} days\"\n",
    "    chapter_times.append({\"chapter\": chapter_title, \"pace\": pace_str})\n",
    "df_chapter = pd.DataFrame(chapter_times)\n",
    "print(\"\\nCHAPTER-WISE ESTIMATED PACE:\")\n",
    "print(df_chapter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f21d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae6bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_pro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a6a0df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langgraph langchain langchain-google-genai langsmith PyMuPDF langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b367ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 1. Imports and Environment ====================\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import TypedDict, Optional, Dict, Any, List\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e4378629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----  Set up API keys and environment variables ----\n",
    "# Replace with your actual keys\n",
    "os.environ[\"GOOGLE_API_KEY\"] = open(\"api_key_paid.txt\").read().strip()\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"ai-tutur-cg\"  # or your project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "388311e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2. PDF to Chunks ----\n",
    "def langchain_load_and_chunk(pdf_path, chunk_size=1200, chunk_overlap=200):\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ed84db83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 3. Build Prompt Template ----\n",
    "prompt_template = \"\"\"\n",
    "You are an expert curriculum designer.\n",
    "Given these document chunks, generate a Learning Path structure in JSON format.\n",
    "\n",
    "Format Example:\n",
    "{{\n",
    "  \"sections\": [\n",
    "    {{\n",
    "      \"section_id\": \"S1\",\n",
    "      \"title\": \"<Section Title>\",\n",
    "      \"brief\": \"<Short 2‚Äì3 line description>\",\n",
    "      \"subsections\": [\n",
    "        {{\n",
    "          \"subsection_id\": \"S1.1\",\n",
    "          \"title\": \"<Subsection Title>\",\n",
    "          \"sub_titles\": [\"<Sub-title 1>\", \"<Sub-title 2>\"],  // Use [] if none\n",
    "          \"brief\": \"<Short 2‚Äì3 line description>\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "For each subsection, include a 'sub_titles' list with relevant sub-headings found in the chunk (use an empty list [] if none are present). Always include the 'brief' field with a 2‚Äì3 line summary.\n",
    "\n",
    "Document Chunks:\n",
    "{chunks}\n",
    "\n",
    "Return only the JSON structure, no additional text or formatting:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8c3e17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 4. Set up Gemini Model ----\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro-latest\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e18b0cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 5. LangGraph Nodes ----\n",
    "from typing import Dict, Any, List, TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    pdf_path: str\n",
    "    docs: List[Any]\n",
    "    chunk_texts: str\n",
    "    skeleton: Dict[str, Any]\n",
    "\n",
    "def load_and_chunk_node(state: State) -> State:\n",
    "    docs = langchain_load_and_chunk(state[\"pdf_path\"])\n",
    "    chunk_texts = \"\\n\\n\".join([f\"Chunk {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
    "    state[\"docs\"] = docs\n",
    "    state[\"chunk_texts\"] = chunk_texts\n",
    "    return state\n",
    "\n",
    "def call_llm_node(state: State) -> State:\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    output = chain.invoke({\"chunks\": state[\"chunk_texts\"]})\n",
    "    # Clean the output to extract JSON from markdown code blocks\n",
    "    def extract_json_from_markdown(text):\n",
    "        \"\"\"Extract JSON content from markdown code blocks\"\"\"\n",
    "        # Try to find JSON within ```json ... ``` blocks\n",
    "        json_pattern = r'```json\\s*(.*?)\\s*```'\n",
    "        match = re.search(json_pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        \n",
    "        # Try to find JSON within ``` ... ``` blocks (without json specifier)\n",
    "        code_pattern = r'```\\s*(.*?)\\s*```'\n",
    "        match = re.search(code_pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            potential_json = match.group(1).strip()\n",
    "            # Check if it looks like JSON (starts with { or [)\n",
    "            if potential_json.startswith(('{', '[')):\n",
    "                return potential_json\n",
    "        \n",
    "        # If no code blocks found, return original text\n",
    "        return text.strip()\n",
    "    \n",
    "    try:\n",
    "        # First, try to extract JSON from markdown\n",
    "        cleaned_output = extract_json_from_markdown(output)\n",
    "        \n",
    "        # Try to parse the cleaned output\n",
    "        skeleton = json.loads(cleaned_output)\n",
    "        \n",
    "        print(\"‚úÖ JSON parsed successfully!\")\n",
    "        print(json.dumps(skeleton, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        # Save to file\n",
    "        with open(\"learning_path_skeleton_ml.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(skeleton, f, indent=2, ensure_ascii=False)\n",
    "        print(\"\\n‚úÖ Learning Skeleton saved as 'learning_path_skeleton_ml.json'\")\n",
    "        \n",
    "        state[\"skeleton\"] = skeleton\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"\\n‚ùå JSON parsing failed: {e}\")\n",
    "        print(\"\\nCleaned output that failed to parse:\")\n",
    "        print(cleaned_output)\n",
    "        print(\"\\nOriginal Gemini output:\")\n",
    "        print(output)\n",
    "        state[\"skeleton\"] = None\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Unexpected error: {e}\")\n",
    "        print(\"\\nOriginal output:\")\n",
    "        print(output)\n",
    "        state[\"skeleton\"] = None\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ab1ae642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 6. Build and Run the LangGraph Workflow ----\n",
    "workflow = (\n",
    "    StateGraph(state_schema=State)\n",
    "    .add_node(\"load_and_chunk\", load_and_chunk_node)\n",
    "    .add_node(\"call_llm\", call_llm_node)\n",
    "    # .add_node(\"save\", save_node)\n",
    "    .add_edge(\"__start__\", \"load_and_chunk\")\n",
    "    .add_edge(\"load_and_chunk\", \"call_llm\")\n",
    "    # .add_edge(\"call_llm\", \"save\")\n",
    "    .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "90d38adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JSON parsed successfully!\n",
      "{\n",
      "  \"sections\": [\n",
      "    {\n",
      "      \"section_id\": \"S1\",\n",
      "      \"title\": \"Preliminaries\",\n",
      "      \"brief\": \"This section introduces the concept of machine learning, its various types (supervised, unsupervised, and speed-up learning), and discusses the importance of bias in learning. It also provides sample applications of machine learning and lists important resources for further exploration.\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"subsection_id\": \"S1.1\",\n",
      "          \"title\": \"Introduction\",\n",
      "          \"sub_titles\": [\n",
      "            \"What is Machine Learning?\",\n",
      "            \"Wellsprings of Machine Learning\",\n",
      "            \"Varieties of Machine Learning\"\n",
      "          ],\n",
      "          \"brief\": \"This subsection defines machine learning, explores its origins in different fields like statistics, brain models, and AI, and discusses the different types of learning, including supervised, unsupervised, and speed-up learning.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S1.2\",\n",
      "          \"title\": \"Learning Input-Output Functions\",\n",
      "          \"sub_titles\": [\n",
      "            \"Types of Learning\",\n",
      "            \"Input Vectors\",\n",
      "            \"Outputs\",\n",
      "            \"Training Regimes\",\n",
      "            \"Noise\",\n",
      "            \"Performance Evaluation\"\n",
      "          ],\n",
      "          \"brief\": \"This subsection delves into the specifics of learning input-output functions, including the types of learning (supervised and unsupervised), different input and output representations, training regimes (batch, incremental, online), the impact of noise, and performance evaluation.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S1.3\",\n",
      "          \"title\": \"Learning Requires Bias\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection explains the crucial role of bias in machine learning, demonstrating how it enables generalization and prevents overfitting by restricting the hypothesis space.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S1.4\",\n",
      "          \"title\": \"Sample Applications\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection provides a brief overview of various real-world applications of machine learning, showcasing its relevance and impact in diverse fields.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S1.5\",\n",
      "          \"title\": \"Sources\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection lists essential resources, including textbooks, papers, conferences, and journals, for further learning and exploration of machine learning.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"section_id\": \"S2\",\n",
      "      \"title\": \"Boolean Functions\",\n",
      "      \"brief\": \"This section provides a comprehensive review of Boolean functions, their representations (algebraic, diagrammatic), and important subclasses (terms, clauses, DNF, CNF, decision lists, symmetric, voting, and linearly separable functions).\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"subsection_id\": \"S2.1\",\n",
      "          \"title\": \"Representation\",\n",
      "          \"sub_titles\": [\n",
      "            \"Boolean Algebra\",\n",
      "            \"Diagrammatic Representations\"\n",
      "          ],\n",
      "          \"brief\": \"This subsection explains how Boolean functions can be represented using Boolean algebra and diagrammatic methods like hypercubes and Karnaugh maps.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S2.2\",\n",
      "          \"title\": \"Classes of Boolean Functions\",\n",
      "          \"sub_titles\": [\n",
      "            \"Terms and Clauses\",\n",
      "            \"DNF Functions\",\n",
      "            \"CNF Functions\",\n",
      "            \"Decision Lists\",\n",
      "            \"Symmetric and Voting Functions\",\n",
      "            \"Linearly Separable Functions\"\n",
      "          ],\n",
      "          \"brief\": \"This subsection explores various important subclasses of Boolean functions, including terms, clauses, DNF, CNF, decision lists, symmetric, voting, and linearly separable functions, and their properties.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"section_id\": \"S3\",\n",
      "      \"title\": \"Using Version Spaces for Learning\",\n",
      "      \"brief\": \"This section introduces the concept of version spaces and version graphs for learning Boolean functions, explaining how they represent the set of consistent hypotheses and how learning can be viewed as a search through this space.\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"subsection_id\": \"S3.1\",\n",
      "          \"title\": \"Version Spaces and Mistake Bounds\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection defines version spaces and mistake bounds, explaining how the size of the version space shrinks as more training examples are presented and how mistake bounds provide theoretical limits on the number of errors a learner can make.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S3.2\",\n",
      "          \"title\": \"Version Graphs\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection introduces version graphs as a way to represent version spaces, showing how hypotheses are ordered by generality and how the graph changes as training examples are presented.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S3.3\",\n",
      "          \"title\": \"Learning as Search of a Version Space\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection explains how learning can be viewed as a search problem within a version space, using specialization and generalization operators to find a consistent hypothesis.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S3.4\",\n",
      "          \"title\": \"The Candidate Elimination Method\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection describes the candidate elimination algorithm, an incremental method for computing the boundary sets of a version space, which represent the most general and most specific consistent hypotheses.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"section_id\": \"S4\",\n",
      "      \"title\": \"Neural Networks\",\n",
      "      \"brief\": \"This section explores the use of neural networks, specifically networks of Threshold Logic Units (TLUs), for implementing and learning various input-output functions. It covers TLU geometry, training methods (error-correction, Widrow-Hoff), and different network architectures (layered, Madalines, piecewise linear, cascade).\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"subsection_id\": \"S4.1\",\n",
      "          \"title\": \"Threshold Logic Units\",\n",
      "          \"sub_titles\": [\n",
      "            \"Definitions and Geometry\",\n",
      "            \"Special Cases of Linearly Separable Functions\",\n",
      "            \"Error-Correction Training of a TLU\",\n",
      "            \"Weight Space\",\n",
      "            \"The Widrow-Hoff Procedure\",\n",
      "            \"Training a TLU on Non-Linearly-Separable Training Sets\"\n",
      "          ],\n",
      "          \"brief\": \"This subsection introduces TLUs, their geometric interpretation as hyperplanes, training methods like error-correction and Widrow-Hoff, the concept of weight space, and strategies for handling non-linearly separable data.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S4.2\",\n",
      "          \"title\": \"Linear Machines\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection describes linear machines, a generalization of TLUs for multi-category classification, and their training using error-correction.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S4.3\",\n",
      "          \"title\": \"Networks of TLUs\",\n",
      "          \"sub_titles\": [\n",
      "            \"Motivation and Examples\",\n",
      "            \"Madalines\",\n",
      "            \"Piecewise Linear Machines\",\n",
      "            \"Cascade Networks\"\n",
      "          ],\n",
      "          \"brief\": \"This subsection explores different architectures of TLU networks, including layered networks, Madalines, piecewise linear machines, and cascade networks, and their motivations and training methods.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S4.4\",\n",
      "          \"title\": \"Training Feedforward Networks by Backpropagation\",\n",
      "          \"sub_titles\": [\n",
      "            \"Notation\",\n",
      "            \"The Backpropagation Method\",\n",
      "            \"Computing Weight Changes in the Final Layer\",\n",
      "            \"Computing Changes to the Weights in Intermediate Layers\",\n",
      "            \"Variations on Backprop\",\n",
      "            \"An Application: Steering a Van\"\n",
      "          ],\n",
      "          \"brief\": \"This subsection explains the backpropagation algorithm for training multilayer feedforward networks, including its notation, derivation, weight update rules, variations like simulated annealing, and a practical application in steering a van.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"section_id\": \"S5\",\n",
      "      \"title\": \"Statistical Learning\",\n",
      "      \"brief\": \"This section covers statistical approaches to learning, including statistical decision theory, Gaussian distributions, and nearest-neighbor methods. It explains how to make decisions based on probability distributions and how to estimate these distributions from data.\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"subsection_id\": \"S5.1\",\n",
      "          \"title\": \"Using Statistical Decision Theory\",\n",
      "          \"sub_titles\": [\n",
      "            \"Background and General Method\",\n",
      "            \"Gaussian (or Normal) Distributions\",\n",
      "            \"Conditionally Independent Binary Components\"\n",
      "          ],\n",
      "          \"brief\": \"This subsection introduces statistical decision theory, explaining how to make optimal decisions based on loss functions, prior probabilities, and likelihoods. It also covers Gaussian distributions and the case of conditionally independent binary components.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S5.2\",\n",
      "          \"title\": \"Learning Belief Networks\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection will cover learning belief networks (to be added).\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S5.3\",\n",
      "          \"title\": \"Nearest-Neighbor Methods\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection describes nearest-neighbor methods for classification, explaining how to classify new patterns based on the classes of their closest neighbors in the training set.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"section_id\": \"S6\",\n",
      "      \"title\": \"Decision Trees\",\n",
      "      \"brief\": \"This section covers decision trees, their definitions, supervised learning methods (including uncertainty reduction and handling non-binary attributes), overfitting and evaluation techniques (cross-validation, MDL), and addresses the problems of replicated subtrees and missing attributes.\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"subsection_id\": \"S6.1\",\n",
      "          \"title\": \"Definitions\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection defines decision trees, their components (tests, leaf nodes), and different types (multivariate, univariate, binary, categorical, numeric).\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S6.2\",\n",
      "          \"title\": \"Supervised Learning of Univariate Decision Trees\",\n",
      "          \"sub_titles\": [\n",
      "            \"Selecting the Type of Test\",\n",
      "            \"Using Uncertainty Reduction to Select Tests\",\n",
      "            \"Non-Binary Attributes\"\n",
      "          ],\n",
      "          \"brief\": \"This subsection explains how to learn univariate decision trees using supervised methods, focusing on uncertainty reduction as a criterion for selecting tests and how to handle non-binary attributes.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S6.3\",\n",
      "          \"title\": \"Networks Equivalent to Decision Trees\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection shows the equivalence between univariate Boolean decision trees and two-layer feedforward neural networks, and between multivariate decision trees and three-layer networks.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S6.4\",\n",
      "          \"title\": \"Overfitting and Evaluation\",\n",
      "          \"sub_titles\": [\n",
      "            \"Overfitting\",\n",
      "            \"Validation Methods\",\n",
      "            \"Avoiding Overfitting in Decision Trees\",\n",
      "            \"Minimum-Description Length Methods\",\n",
      "            \"Noise in Data\"\n",
      "          ],\n",
      "          \"brief\": \"This subsection discusses the problem of overfitting in decision trees, evaluation methods like cross-validation and minimum description length (MDL), and techniques for avoiding overfitting, such as pruning and handling noise.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S6.5\",\n",
      "          \"title\": \"The Problem of Replicated Subtrees\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection addresses the problem of replicated subtrees in decision trees, explaining how it can lead to inefficiency and suggesting solutions like decision graphs and multivariate tests.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S6.6\",\n",
      "          \"title\": \"The Problem of Missing Attributes\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection will address the problem of missing attributes (to be added).\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S6.7\",\n",
      "          \"title\": \"Comparisons\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection compares decision trees with other classifiers like neural networks and nearest-neighbor methods, highlighting their relative strengths and weaknesses.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"section_id\": \"S7\",\n",
      "      \"title\": \"Inductive Logic Programming\",\n",
      "      \"brief\": \"This section introduces Inductive Logic Programming (ILP), a method for learning logic programs from examples and background knowledge. It covers notation, definitions (sufficient, necessary, consistent programs), a generic ILP algorithm, inducing recursive programs, and choosing literals to add.\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"subsection_id\": \"S7.1\",\n",
      "          \"title\": \"Notation and Definitions\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection introduces the notation and definitions used in ILP, including the concepts of covering, sufficient, necessary, and consistent programs.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S7.2\",\n",
      "          \"title\": \"A Generic ILP Algorithm\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection presents a generic ILP algorithm that iteratively adds clauses to a logic program to make it more sufficient while ensuring each clause is necessary.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S7.3\",\n",
      "          \"title\": \"An Example\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection provides a detailed example of how the generic ILP algorithm works, using an airline route map to illustrate the process of inducing a logic program for nonstop flights.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S7.4\",\n",
      "          \"title\": \"Inducing Recursive Programs\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection extends the ILP algorithm to handle recursive programs, using an example of an airline route map with bus routes to illustrate the process.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S7.5\",\n",
      "          \"title\": \"Choosing Literals to Add\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection discusses how to choose literals to add to a clause during the ILP process, using an information-like measure based on the odds of covering positive instances.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S7.6\",\n",
      "          \"title\": \"Relationships Between ILP and Decision Tree Induction\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection explains the relationship between ILP and decision tree induction, showing how the generic ILP algorithm can be viewed as a type of decision tree induction with multivariate splits based on background relations.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"section_id\": \"S8\",\n",
      "      \"title\": \"Computational Learning Theory\",\n",
      "      \"brief\": \"This section introduces Probably Approximately Correct (PAC) learning theory, covering notation, assumptions, the fundamental theorem, examples (terms, linearly separable functions), properly PAC-learnable classes, and the Vapnik-Chervonenkis (VC) dimension.\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"subsection_id\": \"S8.1\",\n",
      "          \"title\": \"Notation and Assumptions for PAC Learning Theory\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection introduces the notation and assumptions used in PAC learning theory, including target function, hypothesis, error, accuracy parameter, confidence parameter, and hypothesis space.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S8.2\",\n",
      "          \"title\": \"PAC Learning\",\n",
      "          \"sub_titles\": [\n",
      "            \"The Fundamental Theorem\",\n",
      "            \"Examples\",\n",
      "            \"Some Properly PAC-Learnable Classes\"\n",
      "          ],\n",
      "          \"brief\": \"This subsection explains the core concepts of PAC learning, including the fundamental theorem, examples of PAC learning with terms and linearly separable functions, and a table of properly PAC-learnable classes.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S8.3\",\n",
      "          \"title\": \"The Vapnik-Chervonenkis Dimension\",\n",
      "          \"sub_titles\": [\n",
      "            \"Linear Dichotomies\",\n",
      "            \"Capacity\",\n",
      "            \"A More General Capacity Result\",\n",
      "            \"Some Facts and Speculations About the VC Dimension\"\n",
      "          ],\n",
      "          \"brief\": \"This subsection introduces the VC dimension as a measure of the expressive power of a hypothesis set, explaining its relationship to linear dichotomies, capacity, and PAC learnability.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S8.4\",\n",
      "          \"title\": \"VC Dimension and PAC Learning\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection connects the VC dimension with PAC learning through two theorems, providing bounds on the number of training patterns needed for PAC learnability.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"section_id\": \"S9\",\n",
      "      \"title\": \"Unsupervised Learning\",\n",
      "      \"brief\": \"This section covers unsupervised learning methods, including clustering and hierarchical clustering. It discusses methods based on Euclidean distance and probabilities, and provides examples of their application.\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"subsection_id\": \"S9.1\",\n",
      "          \"title\": \"What is Unsupervised Learning?\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection defines unsupervised learning as the process of finding natural partitions of patterns, including clustering and hierarchical clustering.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S9.2\",\n",
      "          \"title\": \"Clustering Methods\",\n",
      "          \"sub_titles\": [\n",
      "            \"A Method Based on Euclidean Distance\",\n",
      "            \"A Method Based on Probabilities\"\n",
      "          ],\n",
      "          \"brief\": \"This subsection describes two clustering methods, one based on Euclidean distance and the other on probabilities, explaining how they group patterns into clusters.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S9.3\",\n",
      "          \"title\": \"Hierarchical Clustering Methods\",\n",
      "          \"sub_titles\": [\n",
      "            \"A Method Based on Euclidean Distance\",\n",
      "            \"A Method Based on Probabilities\"\n",
      "          ],\n",
      "          \"brief\": \"This subsection describes two hierarchical clustering methods, one based on Euclidean distance and the other on probabilities, explaining how they create hierarchies of clusters.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"section_id\": \"S10\",\n",
      "      \"title\": \"Temporal-Difference Learning\",\n",
      "      \"brief\": \"This section covers temporal-difference (TD) learning, a method for predicting future values in temporal sequences. It discusses supervised and TD methods, incremental computation, an experiment with TD methods, theoretical results, and intra-sequence weight updating.\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"subsection_id\": \"S10.1\",\n",
      "          \"title\": \"Temporal Patterns and Prediction Problems\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection introduces temporal patterns and prediction problems, distinguishing between predicting the next value and multi-step prediction.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S10.2\",\n",
      "          \"title\": \"Supervised and Temporal-Difference Methods\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection compares supervised learning and TD learning for prediction problems, explaining how TD learning uses differences between successive predictions.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S10.3\",\n",
      "          \"title\": \"Incremental Computation of the (‚àÜW)i\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection describes an incremental method for computing weight changes in TD learning, which saves memory and computation.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S10.4\",\n",
      "          \"title\": \"An Experiment with TD Methods\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection presents an experiment comparing TD methods with supervised learning on a random walk problem, demonstrating the advantages of TD learning in dynamic environments.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S10.5\",\n",
      "          \"title\": \"Theoretical Results\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection presents theoretical results on the convergence of TD(0) and TD(Œª) methods for Markov processes.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S10.6\",\n",
      "          \"title\": \"Intra-Sequence Weight Updating\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection discusses intra-sequence weight updating in TD learning, explaining how to update weights after every pattern presentation rather than after an entire sequence.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S10.7\",\n",
      "          \"title\": \"An Example Application: TD-gammon\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection describes TD-gammon, a program that learns to play backgammon by training a neural network using TD learning and backpropagation.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"section_id\": \"S11\",\n",
      "      \"title\": \"Delayed-Reinforcement Learning\",\n",
      "      \"brief\": \"This section covers delayed-reinforcement learning, where an agent learns to maximize rewards over time by trial and error. It discusses the general problem, an example using a grid world, temporal discounting, optimal policies, Q-learning, limitations, and extensions.\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"subsection_id\": \"S11.1\",\n",
      "          \"title\": \"The General Problem\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection introduces the general problem of delayed-reinforcement learning, where an agent learns to choose actions to maximize rewards in an unknown environment.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S11.2\",\n",
      "          \"title\": \"An Example\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection provides an example of delayed-reinforcement learning using a grid world, illustrating the concepts of states, actions, rewards, and policies.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S11.3\",\n",
      "          \"title\": \"Temporal Discounting and Optimal Policies\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection explains temporal discounting and optimal policies in reinforcement learning, introducing the discount factor and the concept of the value of a policy.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S11.4\",\n",
      "          \"title\": \"Q-Learning\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection describes Q-learning, an incremental dynamic programming method for learning optimal policies in reinforcement learning.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S11.5\",\n",
      "          \"title\": \"Discussion, Limitations, and Extensions of Q-Learning\",\n",
      "          \"sub_titles\": [\n",
      "            \"An Illustrative Example\",\n",
      "            \"Using Random Actions\",\n",
      "            \"Generalizing Over Inputs\",\n",
      "            \"Partially Observable States\",\n",
      "            \"Scaling Problems\"\n",
      "          ],\n",
      "          \"brief\": \"This subsection discusses the limitations and extensions of Q-learning, including an illustrative example, using random actions for exploration, generalizing over inputs with neural networks, handling partially observable states, and addressing scaling problems.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"section_id\": \"S12\",\n",
      "      \"title\": \"Explanation-Based Learning\",\n",
      "      \"brief\": \"This section covers explanation-based learning (EBL), a method for converting implicit knowledge into explicit knowledge. It discusses deductive learning, domain theories, an example, evaluable predicates, more general proofs, the utility of EBL, and applications.\",\n",
      "      \"subsections\": [\n",
      "        {\n",
      "          \"subsection_id\": \"S12.1\",\n",
      "          \"title\": \"Deductive Learning\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection introduces deductive learning, contrasting it with inductive learning and explaining how it involves deriving logical conclusions from facts.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S12.2\",\n",
      "          \"title\": \"Domain Theories\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection explains the role of domain theories in EBL, which provide a priori information about the problem domain.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S12.3\",\n",
      "          \"title\": \"An Example\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection provides an example of EBL, illustrating how a general rule can be derived from a specific example and a domain theory.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S12.4\",\n",
      "          \"title\": \"Evaluable Predicates\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection discusses the concept of evaluable predicates in EBL, which correspond to features that can be directly observed or evaluated.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S12.5\",\n",
      "          \"title\": \"More General Proofs\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection discusses how to generalize proofs in EBL to create more general rules, including structural generalization via disjunctive augmentation.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S12.6\",\n",
      "          \"title\": \"Utility of EBL\",\n",
      "          \"sub_titles\": [],\n",
      "          \"brief\": \"This subsection discusses the utility of EBL, considering the trade-off between adding new rules and increasing the size of the domain theory.\"\n",
      "        },\n",
      "        {\n",
      "          \"subsection_id\": \"S12.7\",\n",
      "          \"title\": \"Applications\",\n",
      "          \"sub_titles\": [\n",
      "            \"Macro-Operators in Planning\",\n",
      "            \"Learning Search Control Knowledge\"\n",
      "          ],\n",
      "          \"brief\": \"This subsection describes two applications of EBL: creating macro-operators in planning and learning search control knowledge.\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "‚úÖ Learning Skeleton saved as 'learning_path_skeleton_ml.json'\n",
      "\n",
      "üéâ Workflow completed! Final skeleton keys: ['sections']\n"
     ]
    }
   ],
   "source": [
    "initial_state = {\n",
    "    \"pdf_path\": \"mi-intro.pdf\",\n",
    "    \"docs\": [],\n",
    "    \"chunk_texts\": \"\",\n",
    "    \"skeleton\": {}\n",
    "}\n",
    "# workflow.invoke(initial_state)\n",
    "\n",
    "result = workflow.invoke(initial_state)\n",
    "print(f\"\\nüéâ Workflow completed! Final skeleton keys: {list(result['skeleton'].keys()) if result['skeleton'] else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091025e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_pro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
